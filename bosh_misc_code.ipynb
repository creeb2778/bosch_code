{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#models\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these vars are determined from the xgb f score script\n",
    "df_n_prep = pd.read_csv('train_numeric.csv',usecols=['Id',\n",
    "             'L3_S32_F3850','L0_S0_F0','L0_S12_F330','L1_S24_F1512','L3_S38_F3952','L3_S33_F3855',\n",
    "             'L1_S24_F1514','L3_S33_F3859','L1_S24_F1118','L1_S25_F1855','L1_S24_F1723','L3_S38_F3960',\n",
    "             'L0_S0_F18','L3_S30_F3494','L0_S0_F8','L0_S0_F2','L2_S27_F3129','L1_S24_F1518','L3_S30_F3749',\n",
    "             'L3_S33_F3865','L2_S27_F3199','L1_S24_F1632','L2_S26_F3106','L3_S29_F3333','L0_S0_F4','L0_S4_F104',\n",
    "             'L0_S12_F346','L1_S24_F1581','L0_S12_F332','L3_S33_F3857',\n",
    "             'Response'], dtype=np.float32 )\n",
    "\n",
    "num_chunks_test = pd.read_csv(\"test_numeric.csv\", \n",
    "                         usecols=['Id',\n",
    "             'L3_S32_F3850','L0_S0_F0','L0_S12_F330','L1_S24_F1512','L3_S38_F3952','L3_S33_F3855',\n",
    "             'L1_S24_F1514','L3_S33_F3859','L1_S24_F1118','L1_S25_F1855','L1_S24_F1723','L3_S38_F3960',\n",
    "             'L0_S0_F18','L3_S30_F3494','L0_S0_F8','L0_S0_F2','L2_S27_F3129','L1_S24_F1518','L3_S30_F3749',\n",
    "             'L3_S33_F3865','L2_S27_F3199','L1_S24_F1632','L2_S26_F3106','L3_S29_F3333','L0_S0_F4','L0_S4_F104',\n",
    "             'L0_S12_F346','L1_S24_F1581','L0_S12_F332','L3_S33_F3857'], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to create graphs, the L0_S0_F0 is the most predictive var want to see it mapped against other vars\n",
    "for feat in df_n_prep.columns[2:]:\n",
    "    sns.factorplot(data = df_n_prep,x='L0_S0_F0',y=feat ,hue='Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used variation of this code to do PCA. You have to change range values depending on the size of your dataset\n",
    "pca = PCA()\n",
    "test_pca = pca.fit_transform(X)\n",
    "pca.explained_variance_ratio_\n",
    "plt.bar(range(1,24),pca.explained_variance_ratio_, alpha=0.5, align='center')\n",
    "plt.step(range(1,24),np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#great code for breaking data into chunks and reading in from large files\n",
    "num_chunks = pd.read_csv(\"train_numeric.csv\", index_col=0,\n",
    "                         usecols=list(range(969)), chunksize=100000, dtype=np.float32)\n",
    "cat_chunks = pd.read_csv(\"train_categorical.csv\", index_col=0,\n",
    "                         usecols=list(range(2140)), chunksize=100000, dtype=str)\n",
    "\n",
    "X = pd.concat([pd.concat([ nchunk, cchunk], axis=1).sample(frac=0.01)\n",
    "               \n",
    "               for nchunk, cchunk in zip(num_chunks,cat_chunks)])\n",
    "y = pd.read_csv(\"train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32).loc[X.index].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alt option for random samp from a dataset\n",
    "# reading in training data\n",
    "filename_num = 'train_numeric.csv'\n",
    "#filename_date = 'train_date.csv'\n",
    "filename_cat = 'train_categorical.csv'\n",
    "n_num = sum(1 for lin in open('train_numeric.csv'))-1\n",
    "\n",
    "s = 40000\n",
    "chunksize_= 30000\n",
    "skip = sorted(random.sample(xrange(1,n_num+1),n_num-s)) #the 0-indexed header will not be included in the skip list\n",
    "df_n_prep = pd.read_csv(filename_num, skiprows=skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modified code to do my own feature selection via the random samp from above \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "\n",
    "# note in the get data fx the first 4 lines were added to speed up process!\n",
    "def ceate_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    i = 0\n",
    "    for feat in features:\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "        i = i + 1\n",
    "\n",
    "    outfile.close()\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    n_num = sum(1 for lin in open('train_categorical.csv'))-1\n",
    "    s = 1000\n",
    "    chunksize_= 30000\n",
    "    skip = sorted(random.sample(xrange(1,n_num+1),n_num-s)) #the 0-indexed header will not be included in the skip list\n",
    "    train = pd.read_csv(\"train_categorical.csv\",skiprows=skip)\n",
    "    X = train.set_index('Id')\n",
    "\n",
    "    \n",
    "    y_train = pd.read_csv(\"train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32).loc[X.index]\n",
    "    y_train= y_train.reset_index()\n",
    "    train = pd.merge(train,y_train, how ='left',on='Id')\n",
    "    features = list(train.columns[1:2141])    \n",
    "\n",
    "    for feat in train.select_dtypes(include=['object']).columns:\n",
    "        m = train.groupby([feat])['Response'].mean()\n",
    "        train[feat].replace(m,inplace=True)\n",
    "\n",
    "    x_train = train[features]\n",
    "\n",
    "    return features, x_train, y_train\n",
    "\n",
    "\n",
    "features, x_train, y_train = get_data()\n",
    "\n",
    "y_train.drop('Id',axis=1,inplace=True)\n",
    "ceate_feature_map(features)\n",
    "\n",
    "xgb_params = {\"objective\": \"reg:linear\", \"eta\": 0.5, \"max_depth\": 4, \"seed\": 42, \"silent\": 1}\n",
    "num_rounds = 500\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "x_train.fillna(0,inplace=True)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "importance = gbdt.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "plt.figure()\n",
    "df.plot()\n",
    "df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.gcf().savefig('feature_importance_xgb.png')\n",
    "df[:100].to_csv('important_features.csv')\n",
    "#sort the top x amount to use\n",
    "df.sort_values('fscore',ascending=False,inplace=True)\n",
    "df[:30].to_csv('important_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the normal feature importance code\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "def ceate_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    i = 0\n",
    "    for feat in features:\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "        i = i + 1\n",
    "\n",
    "    outfile.close()\n",
    "\n",
    "def get_data():\n",
    "    train = pd.read_csv(\"train_numeric.csv\",dtype=np.float32)\n",
    "\n",
    "    features = list(train.columns[1:969])\n",
    "\n",
    "    y_train = train.Response\n",
    "\n",
    "    for feat in train.select_dtypes(include=['object']).columns:\n",
    "        m = train.groupby([feat])['Response'].mean()\n",
    "        train[feat].replace(m,inplace=True)\n",
    "\n",
    "    x_train = train[features]\n",
    "\n",
    "    return features, x_train, y_train\n",
    "\n",
    "\n",
    "features, x_train, y_train = get_data()\n",
    "ceate_feature_map(features)\n",
    "\n",
    "xgb_params = {\"objective\": \"reg:linear\", \"eta\": 0.01, \"max_depth\": 4, \"seed\": 42, \"silent\": 1}\n",
    "num_rounds = 1000\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "x_train.fillna(0,inplace=True)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "gbdt = xgb.train(xgb_params, dtrain, num_rounds)\n",
    "\n",
    "importance = gbdt.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "plt.figure()\n",
    "df.plot()\n",
    "df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.gcf().savefig('feature_importance_xgb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here is LSTM RNN code, had to give up because it took to long to run\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "theano.config.floatX = 'float32'\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "t1 = test2.iloc[:,1:]\n",
    "X= X.astype(theano.config.floatX)\n",
    "t1 = t1.astype(theano.config.floatX)\n",
    "\n",
    "#code to impute and normalize\n",
    "\n",
    "#play wish shape of tensor\n",
    "X = np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "t2 = np.reshape(t2,(t2.shape[0],t2.shape[1],1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(5,input_dim=X.shape[2],return_sequences=True))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print('fitting')\n",
    "model.fit(X,y_train, nb_epoch=15, batch_size=100000,validation_split=0.1)\n",
    "\n",
    "pred = model.predict(t2,batch_size=100000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
