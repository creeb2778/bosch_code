{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Charles/anaconda/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "/Users/Charles/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Charles/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#import utility modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "#models\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from theano import tensor as T\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import theano\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these vars are determined from the xgb f score script\n",
    "df_n_prep = pd.read_csv('train_numeric.csv',usecols=['Id',\n",
    "'L3_S32_F3850',\n",
    "'L0_S0_F0',\n",
    "'L0_S12_F330',\n",
    "'L1_S24_F1512',\n",
    "'L3_S38_F3952',\n",
    "'L3_S33_F3855',\n",
    "'L0_S0_F2',\n",
    "'L3_S33_F3859',\n",
    "'L3_S33_F3857',\n",
    "'L1_S24_F1514',\n",
    "'L0_S4_F104',\n",
    "'L3_S29_F3351',\n",
    "'L0_S12_F334',\n",
    "'L1_S24_F1723',\n",
    "'L1_S24_F1118',\n",
    "'L0_S1_F28',\n",
    "'L0_S0_F20',\n",
    "'L3_S29_F3339',\n",
    "'L0_S5_F114',\n",
    "'L3_S30_F3494',\n",
    "'L1_S25_F1855',\n",
    "'L3_S33_F3865',\n",
    "'L3_S36_F3920',\n",
    "'L0_S0_F4',\n",
    "'L2_S26_F3069',\n",
    "'L0_S2_F36',\n",
    "'L0_S0_F16',\n",
    "'L3_S29_F3345',\n",
    "'L3_S30_F3759',\n",
    "'L1_S24_F1516',\n",
    "'L2_S27_F3199',\n",
    "'L3_S29_F3333',\n",
    "'L2_S26_F3047',\n",
    "'L0_S13_F356',\n",
    "'L3_S30_F3754',\n",
    "'L2_S26_F3106',\n",
    "'L3_S38_F3960',\n",
    "'L2_S27_F3129',\n",
    "'L3_S38_F3956',\n",
    "'L0_S9_F160',\n",
    "'L2_S26_F3117',\n",
    "'L2_S26_F3121',\n",
    "'L1_S24_F1581',\n",
    "'L3_S30_F3804',\n",
    "'L0_S3_F72',\n",
    "'L3_S30_F3769',\n",
    "'L0_S3_F80',\n",
    "'L3_S30_F3749',\n",
    "'L2_S26_F3036',\n",
    "'L0_S12_F332',\n",
    "'L1_S24_F1518',\n",
    "'L0_S0_F8',\n",
    "             'Response'], dtype=np.float32 )\n",
    "\n",
    "num_chunks_test = pd.read_csv(\"test_numeric.csv\", \n",
    "                         usecols=['Id',\n",
    "             'L3_S32_F3850',\n",
    "'L0_S0_F0',\n",
    "'L0_S12_F330',\n",
    "'L1_S24_F1512',\n",
    "'L3_S38_F3952',\n",
    "'L3_S33_F3855',\n",
    "'L0_S0_F2',\n",
    "'L3_S33_F3859',\n",
    "'L3_S33_F3857',\n",
    "'L1_S24_F1514',\n",
    "'L0_S4_F104',\n",
    "'L3_S29_F3351',\n",
    "'L0_S12_F334',\n",
    "'L1_S24_F1723',\n",
    "'L1_S24_F1118',\n",
    "'L0_S1_F28',\n",
    "'L0_S0_F20',\n",
    "'L3_S29_F3339',\n",
    "'L0_S5_F114',\n",
    "'L3_S30_F3494',\n",
    "'L1_S25_F1855',\n",
    "'L3_S33_F3865',\n",
    "'L3_S36_F3920',\n",
    "'L0_S0_F4',\n",
    "'L2_S26_F3069',\n",
    "'L0_S2_F36',\n",
    "'L0_S0_F16',\n",
    "'L3_S29_F3345',\n",
    "'L3_S30_F3759',\n",
    "'L1_S24_F1516',\n",
    "'L2_S27_F3199',\n",
    "'L3_S29_F3333',\n",
    "'L2_S26_F3047',\n",
    "'L0_S13_F356',\n",
    "'L3_S30_F3754',\n",
    "'L2_S26_F3106',\n",
    "'L3_S38_F3960',\n",
    "'L2_S27_F3129',\n",
    "'L3_S38_F3956',\n",
    "'L0_S9_F160',\n",
    "'L2_S26_F3117',\n",
    "'L2_S26_F3121',\n",
    "'L1_S24_F1581',\n",
    "'L3_S30_F3804',\n",
    "'L0_S3_F72',\n",
    "'L3_S30_F3769',\n",
    "'L0_S3_F80',\n",
    "'L3_S30_F3749',\n",
    "'L2_S26_F3036',\n",
    "'L0_S12_F332',\n",
    "'L1_S24_F1518',\n",
    "'L0_S0_F8'], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to create graphs, the L0_S0_F0 is the most predictive var\n",
    "for feat in df_n_prep.columns[2:]:\n",
    "    sns.factorplot(data = df_n_prep,x='L0_S0_F0',y=feat ,hue='Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#used to grab other important vars from a script on kaggle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "DATA_DIR = \"\"\n",
    "\n",
    "ID_COLUMN = 'Id'\n",
    "TARGET_COLUMN = 'Response'\n",
    "\n",
    "SEED = 0\n",
    "CHUNKSIZE = 50000\n",
    "NROWS = 250000\n",
    "\n",
    "TRAIN_NUMERIC = \"train_numeric.csv\".format(DATA_DIR)\n",
    "TRAIN_DATE = \"train_date.csv\".format(DATA_DIR)\n",
    "\n",
    "TEST_NUMERIC = \"test_numeric.csv\".format(DATA_DIR)\n",
    "TEST_DATE = \"test_date.csv\".format(DATA_DIR)\n",
    "\n",
    "FILENAME = \"etimelhoods\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN])\n",
    "test = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN])\n",
    "\n",
    "train[\"StartTime\"] = -1\n",
    "test[\"StartTime\"] = -1\n",
    "\n",
    "\n",
    "nrows = 0\n",
    "for tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):\n",
    "    feats = np.setdiff1d(tr.columns, [ID_COLUMN])\n",
    "\n",
    "    stime_tr = tr[feats].min(axis=1).values\n",
    "    stime_te = te[feats].min(axis=1).values\n",
    "\n",
    "    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n",
    "    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n",
    "\n",
    "#    nrows += CHUNKSIZE\n",
    "#    if nrows >= NROWS:\n",
    "#        break\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n",
    "\n",
    "train_test['z1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "train_test['z2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n",
    "\n",
    "train_test['z3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "train_test['z4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\n",
    "train = train_test.iloc[:ntrain, :]\n",
    "\n",
    "features = np.setdiff1d(list(train.columns), [TARGET_COLUMN, ID_COLUMN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.merge(df_n_prep,train_test,how='left',on='Id')\n",
    "test2 = pd.merge(num_chunks_test,train_test, how='left',on='Id')\n",
    "\n",
    "X.drop(['Id','Response_x','Response_y'],axis=1,inplace=True)\n",
    "test2.drop(['Response','Id'],axis=1,inplace=True)\n",
    "y_hold = pd.read_csv(\"train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32)\n",
    "y_train = y_hold['Response']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## beat .37808\n",
    "#0.37415\n",
    "imp = Imputer()\n",
    "scl = StandardScaler()\n",
    "\n",
    "X = imp.fit_transform(X)\n",
    "X = scl.fit_transform(X)\n",
    "y_hold = pd.read_csv(\"train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32)\n",
    "y_train = y_hold['Response']\n",
    "\n",
    "boost = xgb.XGBClassifier(max_depth = 8, learning_rate = 0.11,n_estimators=500,\n",
    "                         min_child_weight=19, silent=1,objective='binary:logistic')\n",
    "\n",
    "boost.fit(X,y_train)\n",
    "\n",
    "t2 = imp.fit_transform(test2)\n",
    "t2 = scl.fit_transform(t2)\n",
    "print('Fitted')\n",
    "pred = boost.predict(t2)\n",
    "Response = pd.Series(pred)\n",
    "#doing these to get correct dtypes in \n",
    "Response2 = pd.Series(np.where(Response==1,1,0))\n",
    "Id = pd.read_csv(\"test_numeric.csv\", usecols=['Id'])\n",
    "\n",
    "sub2 = pd.concat([Id,Response2],names=True,axis=1)\n",
    "sub2.columns = ['Id','Response']\n",
    "sub2.to_csv('new_tune_money4.csv',index=False)\n",
    "sum(sub2.Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best so far\n",
    "\n",
    "boost = xgb.XGBClassifier(max_depth = 9, learning_rate = 0.099,n_estimators=700,\n",
    "                         min_child_weight=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boost.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
